

**生成 README.md 文件**

看起來，使用者想在生成的 README.md 文件中加入額外符號來避免格式錯亂。我的理解是，可能需要額外的縮排、代碼區塊或使用特殊符號來防止格式顯示錯誤，例如加上 “```text” 或使用 "::::"。我應該選擇將內容包裹在代碼區塊中來保證格式正確。


---

**生成 README.md 文件**

我需要創建一個詳盡的 README 文件，包括環境設置、數據結構、配置 YAML 解釋、數據集準備命令等。同時，考慮到使用者需要打印或發送文件，使用 canvas 創建長文件可能會比在聊天中直接顯示更合適。這樣更便於用戶迭代和打印。


---

**生成 README.md 文件**

用戶希望生成 README 文件，並避免格式錯亂。所以，我認為直接在消息中提供 README 內容是較安全的做法。我可以在代碼區塊內使用普通的 markdown，為了避免格式問題，可以加上一層額外的 "#" 來保持一致的格式。


---

**生成 README.md 文件**

為了避免格式錯亂，我會將 README 內容放在代碼區塊中。這份 README 需要詳細說明步驟：環境設置、配置、數據集處理，label 扫描，如何使用 PEFT 進行推理，以及提示和故障排除等。我還會提到 GPU 設置、參數組選項和保存的檢查點結構等信息。
</think>

已思考 21s
<br/>

好的！我先把一份可直接用的 **README.md** 生成出来。为避免你的输出面板格式错乱，我把全文放进代码块里（等你复制到仓库后就是标准 Markdown 了）。

```markdown
# Qwen2.5-VL ROI 微调项目（整图 + 多ROI 判读）

> 目标：基于**异常图配同名 txt（多框、多类）** + **正常图无标注**的数据形态，构建“整图 + `<regions>` 候选”的多模态 SFT 数据，并采用 **PEFT/QLoRA** 逐步微调 Qwen2.5-VL：  
> Phase-1 冻结视觉塔，仅训 projector + 语言侧 LoRA → Phase-2 视觉顶层加 LoRA → Phase-3 顶层小步长解冻（可选）。  
> 同时提供**盲测**（train/val/test 三分）与 **checkpoint 最优保存** 机制，方便你把最佳权重带回上一个项目测试。

---

## 目录结构建议

```
qwenvl_roi_finetune/
├─ README.md
├─ configs/
│  ├─ data.yaml            # 数据生成/切分配置（本README详解）
│  ├─ train_phase1.yaml    # 冻视觉塔，仅训 projector + LM LoRA
│  ├─ train_phase2.yaml    # 视觉顶层N个block加LoRA
│  └─ train_phase3.yaml    # 顶层小步长解冻（可选）
├─ scripts/
│  ├─ scan_labels.py       # 扫描异常txt第三列标签分布，生成 label_map 建议
│  ├─ prepare_regions.py   # (train/val/test) 生成 regions_*.jsonl
│  ├─ build_sft_jsonl.py   # 将 regions_*.jsonl → sft_*.jsonl（messages）
│  ├─ train.py             # 训练入口（保存最优ckpt，支持resume）
│  └─ eval.py              # （可选）评测脚本（ROI级/图像级指标）
├─ src/
│  ├─ data/…               # 数据读取与 collator（如需细化）
│  ├─ modeling/
│  │  ├─ load_qwen_vl.py   # 模型装载/冻结/LoRA注入
│  │  └─ lora_targets.py   # （可选）LoRA目标层枚举
│  └─ runtime/infer.py     # （可选）推理封装
└─ data/
   ├─ abnormal_root/       # 异常图（每张配同名 .txt）
   ├─ normal_root/         # 正常图（无txt）
   └─ prepared/            # 脚本输出（regions_*.jsonl / sft_*.jsonl 等）
```

---

## 快速上手 TL;DR

```bash
# 0) 环境
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -U "transformers>=4.41" "datasets>=2.19" "peft>=0.11" accelerate pyyaml pillow

# 1) 放数据
# data/abnormal_root/ 里：每张异常图旁边有同名 .txt
# data/normal_root/    里：正常图（无txt）

# 2) 统计标签第三列（可生成 label_map 建议）
python scripts/scan_labels.py --abnormal_dir ./data/abnormal_root --column_delim ";"

# 3) 编辑 configs/data.yaml（见下方模板），填好 label_map / holdout_list 等

# 4) 切分并生成候选（train/val/test）
python scripts/prepare_regions.py --config configs/data.yaml

# 5) 构建多模态SFT样本（messages）
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_train.jsonl --out_jsonl ./data/prepared/sft_train.jsonl
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_val.jsonl   --out_jsonl ./data/prepared/sft_val.jsonl
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_test.jsonl  --out_jsonl ./data/prepared/sft_test.jsonl   # 盲测，先别参与训练

# 6) 训练 Phase-1（冻结视觉塔，仅训 projector + LM LoRA）
python scripts/train.py --train_config configs/train_phase1.yaml
# 断点续训：
python scripts/train.py --train_config configs/train_phase1.yaml --resume

# 7) （可选）Phase-2/3
python scripts/train.py --train_config configs/train_phase2.yaml
python scripts/train.py --train_config configs/train_phase3.yaml

# 8) 用最优 LoRA 权重在你“上一个项目”里做盲测（载入 base + adapter）
# 详见“推理/复用 checkpoint”一节
```

---

## 数据与标注约定

- **异常图**：每张图像旁边有**同名 .txt**；每一行**三列**：  
  `filename ; coords_field ; defect_type`  
  - 列分隔符默认为 `;`，**支持全角 `；`**。  
  - **第二列 coords_field 内部也可能包含分号/逗号/空格**，同时可能包含**多个框**，如：  
    `1190；206；1712；377  1813；555；1866；636`  
    解析时会把这些分隔符统一，并**每 4 个数字一组** → 一个框。  
  - 坐标默认是 **x1,y1,x2,y2**（如为 cx,cy,w,h，可在配置里切换）。
- **正常图**：无标注；训练/推理统一使用**干扰框**（随机生成 K 个）。

> 训练与推理都不在图上画框，而是把候选区域写成 `<regions>` 文本传入，避免“有框=异常”的捷径。

---

## `configs/data.yaml` 模板（可直接复制）

```yaml
# --- 基础路径 ---
abnormal_dir: ./data/abnormal_root
normal_dir:   ./data/normal_root
out_dir:      ./data/prepared

# --- 随机种子 & 划分比例（train/val/test 三分）---
seed: 42
train_ratio: 0.8
val_ratio:   0.1
test_ratio:  0.1

# --- 防泄漏 ---
split_by_parent: true        # 同一父目录尽量进同一子集（按批次/场景分组）
holdout_list: []             # 显式点名若干图片强制进 test（文件名，不含路径）

# --- 候选框策略 ---
min_box_size: 24             # 像素，过小过滤
max_boxes_per_image: 8       # 每图最多 K 个候选（推理时延友好）
abnormal_neg_per_pos: 1      # 异常图：每个正ROI配多少负ROI → 1:1
normal_decoys_per_image: 6   # 正常图：随机干扰框数

# --- TXT 解析 ---
column_delim: ";"            # 三列之间分隔符（支持全角；）
inner_coord_delims: ["；",";","，",","," "]  # 第二列内部的坐标分隔
coords_are_xyxy: true        # 若是 cx,cy,w,h，请改为 false

# --- 标签映射（先跑 scan_labels.py，会给出建议段，粘贴到这里）---
label_map:
  # 例：
  # crack: 裂缝
  # seepage: 渗漏
  # spall: 掉块
```

> 流程建议：**先运行 `scan_labels.py`** 获取第三列标签分布与 `label_map` 建议段，再粘贴到此处，确保类别统一。

---

## 数据准备命令

```bash
# 统计第三列标签（生成 ./data/prepared/label_stats.json 并在终端给出 label_map 建议）
python scripts/scan_labels.py --abnormal_dir ./data/abnormal_root --column_delim ";"

# 依据 data.yaml 切分 + 生成三份 regions_*.jsonl
python scripts/prepare_regions.py --config configs/data.yaml

# 将 regions_*.jsonl 转为多模态 SFT（messages）
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_train.jsonl --out_jsonl ./data/prepared/sft_train.jsonl
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_val.jsonl   --out_jsonl ./data/prepared/sft_val.jsonl
python scripts/build_sft_jsonl.py --regions_jsonl ./data/prepared/regions_test.jsonl  --out_jsonl ./data/prepared/sft_test.jsonl
```

---

## 训练配置说明

### `configs/train_phase1.yaml`（推荐先跑）

- 冻结视觉塔（ViT/SigLIP 等）；  
- LoRA/QLoRA 挂在：**多模态 projector** + 语言侧中间层（q/k/v/o + MLP）；  
- 自动**保存最优 checkpoint**（以 `eval_loss` 作为指标）并支持 `--resume`。

示例要点：
```yaml
model_name: Qwen/Qwen2.5-VL-7B-Instruct
output_dir: ./runs/phase1
precision: bf16
image_short_side: 1024
max_regions_per_image: 8

peft:
  enable: true
  qlora: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj","mm_projector"]

freeze:
  vision_tower: true
  projector: false
  language: lora_only

optim:
  lr_lm: 1.0e-4
  lr_proj: 2.0e-4
  batch_size: 1
  grad_accum: 8
  epochs: 2
  warmup_steps: 200
  weight_decay: 0.01
  grad_checkpointing: true

data:
  train_jsonl: ./data/prepared/sft_train.jsonl
  val_jsonl:   ./data/prepared/sft_val.jsonl

trainer:
  save_strategy: steps
  save_steps: 200
  evaluation_strategy: steps
  eval_steps: 200
  logging_steps: 50
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_safetensors: true
```

### Phase-2 / Phase-3
- **Phase-2**：在视觉塔**顶层 N 个 block** 上加 LoRA（`lr_vision_lora≈5e-5`）。  
- **Phase-3**：只**解冻**最顶 1–2 个 block，小步长微调（`~1e-5`）。

---

## 训练命令

```bash
# Phase-1
python scripts/train.py --train_config configs/train_phase1.yaml

# 断点续训
python scripts/train.py --train_config configs/train_phase1.yaml --resume

# Phase-2（可选）
python scripts/train.py --train_config configs/train_phase2.yaml

# Phase-3（可选）
python scripts/train.py --train_config configs/train_phase3.yaml
```

**输出结构**（示例）：
```
runs/phase1/
  checkpoint-0400/
    adapter_config.json
    adapter_model.safetensors     # ← LoRA/QLoRA 权重
  checkpoint-0600/ ...
  ...
```
> 已启用 `load_best_model_at_end: true`，训练完会把 **eval_loss 最低** 的权重自动还原到内存；磁盘保留若干中间 ckpt。

---

## 推理 / 复用 checkpoint（在“上一个项目”中）

> 若为 LoRA/QLoRA：加载“基座 + 适配器”即可。

```python
from transformers import AutoProcessor, AutoModelForCausalLM # 或 AutoModelForVision2Seq，按基座而定
from peft import PeftModel
import torch, json

base = "Qwen/Qwen2.5-VL-7B-Instruct"
ckpt = "runs/phase1/checkpoint-XXXX"  # 选择最优的那个目录

processor = AutoProcessor.from_pretrained(base, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, ckpt)
model.eval()

# 构造 messages（与训练一致：system约束 + user包含 <regions> + image）
# 读取 ./data/prepared/sft_test.jsonl，逐条把 user 的 content 喂给模型生成
```

> **注意**：推理模板与训练保持一致；输入整图 + `<regions>` 文本（不在像素上画框）。  
> 若需要统一阈值：在验证集上扫 `confidence`（如 0.4~0.8），选 F1 最优点固化到推理配置。

---

## 评测（可选）

- **ROI 级**：对齐 `positive_region_ids` 计算 Precision/Recall/F1；  
- **图像级**：任一 ROI 异常 → 图像判异常，算 P/R/F1、AUROC；  
- 验证/测试划分默认 **按父目录分组**（`split_by_parent`），尽量避免同场景泄漏；还可用 `holdout_list` 指定盲测样本。

> 运行：`python scripts/eval.py --config configs/data.yaml --ckpt runs/phase1/checkpoint-XXXX`

---

## 常见问题（FAQ）

- **第二列坐标分隔符很乱，怎么办？**  
  在 `data.yaml` 中 `inner_coord_delims` 已支持 `["；",";","，",","," "]`；脚本会统一并按**每4个数字**切框。

- **为什么要给正常图加“干扰框”？**  
  为了让模型学习“在候选中辨别异常”，避免“有框=异常”的捷径，并与推理阶段对齐。

- **什么时候考虑动视觉塔？**  
  Phase-1 跑完若 ROI F1 仍被“纹理域差异”限制，可在 Phase-2 给**顶层少量 block**加 LoRA；仍不足再试 Phase-3 小步长解冻最顶 1–2 个 block。

- **显存不够**：  
  开 `peft.qlora=true`、减小 `peft.r`（如 8/16）、`optim.grad_checkpointing=true`、降低 `image_short_side`（如 896/768）。

---

## 版本与环境

- Python 3.10+  
- transformers ≥ 4.41  
- peft ≥ 0.11  
- datasets ≥ 2.19  
- accelerate（按需配置）  

> NVIDIA GPU 建议 24GB+（QLoRA 可显著降低占用）。

---

## 变更记录（可按需维护）
- v0.1 初始化：数据切分/解析（支持全/半角分号 & 多框）、SFT 构建、Phase-1/2/3 训练、最优 ckpt 保存、盲测集支持。
```

